{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "from tensorflow import shape as tf_shape\n",
    "from tensorflow import exp as tf_exp\n",
    "from tensorflow import square as tf_square\n",
    "from tensorflow import reduce_sum, reduce_mean\n",
    "from tensorflow import GradientTape\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Layer, Input, Dense, Conv2D, Conv2DTranspose, Flatten, Reshape, MaxPooling2D, UpSampling2D, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.metrics import Mean\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "from tensorflow.keras.backend import random_normal\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n",
    "# Sampling layer\n",
    "class Sampling(Layer):\n",
    "    \"used to sample a vector in latent space with learned mean - z_mean and (log) variance - z_log_var\"\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch_size = tf_shape(z_mean)[0]\n",
    "        vec_len = tf_shape(z_mean)[1]\n",
    "        epsilon = random_normal(shape=(batch_size, vec_len))\n",
    "        return z_mean + tf_exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "\n",
    "# Define the encoder model\n",
    "def encoder_model(input_shape, filters, dense_layer_dim, latent_dim):\n",
    "    \"\"\"\n",
    "    Creates an encoder model for grayscale images that maps input images to a lower-dimensional latent space.\n",
    "    \n",
    "    Args:\n",
    "    - input_shape: Tuple representing the shape of the input images (height, width, channels).\n",
    "    - filters: List of integers representing the number of filters in each convolutional layer.\n",
    "    - dense_layer_dim: Integer representing the number of neurons in the dense layer.\n",
    "    - latent_dim: Integer representing the dimensionality of the latent space.\n",
    "    \n",
    "    Returns:\n",
    "    - encoder: Keras Model object representing the encoder model.\n",
    "    - encoder_layers_dim: List of tuples representing the dimensionality of each layer in the encoder.\n",
    "    \"\"\"\n",
    "    # Create input layer\n",
    "    encoder_layers_dim = []  # List to store the dimensions of each layer in the encoder\n",
    "    \n",
    "    # Define the input layer\n",
    "    encoder_inputs = Input(shape=input_shape)\n",
    "    encoder_layers_dim.append(tuple(encoder_inputs.shape[1:]))  # Add input layer dimensions to list\n",
    "    \n",
    "    # Add convolutional layers with specified number of filters and activation function\n",
    "    x = Conv2D(filters[0], (3,3), activation=\"relu\", strides=2, padding=\"same\")(encoder_inputs)\n",
    "    encoder_layers_dim.append(tuple(x.shape[1:]))  # Add conv layer dimensions to list\n",
    "    \n",
    "    # Add additional convolutional layers with specified number of filters and activation function\n",
    "    mid_layers = [Conv2D(f, 3, activation=\"relu\", strides=2, padding=\"same\") for f in filters[1:]]\n",
    "    for mid_layer in mid_layers:\n",
    "        x = mid_layer(x)\n",
    "        encoder_layers_dim.append(tuple(x.shape[1:]))  # Add mid layer dimensions to list\n",
    "    \n",
    "    # Flatten convolutional output to prepare for dense layers\n",
    "    x = Flatten()(x)\n",
    "    encoder_layers_dim.append(tuple(x.shape[1:]))  # Add flattened layer dimensions to list\n",
    "    \n",
    "    # Add dense layer with specified number of neurons and activation function\n",
    "    x = Dense(dense_layer_dim, activation='relu')(x)\n",
    "    \n",
    "    # Add output layers for latent space (mean and variance) and sample from this space\n",
    "    z_mean = Dense(latent_dim, name = \"z_mean\")(x)\n",
    "    z_log_var = Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "    z = Sampling()([z_mean, z_log_var])\n",
    "    encoder_layers_dim.append(tuple(z.shape[1:]))  # Add output layer dimensions to list\n",
    "    \n",
    "    # Create encoder model\n",
    "    return Model(encoder_inputs, [z_mean, z_log_var, z], name='encoder'), encoder_layers_dim\n",
    "\n",
    "# decoder model for grayscale images\n",
    "def decoder_model(encoder_layers_dim):\n",
    "    # Extract necessary dimensions from encoder model output\n",
    "    latent_dim = encoder_layers_dim[-1][0]\n",
    "    dense_layer_dim = encoder_layers_dim[-2][0]\n",
    "    first_conv_layer_dim = encoder_layers_dim[-3]\n",
    "    output_layer = encoder_layers_dim[0]\n",
    "\n",
    "    # Create input layer for latent space vector\n",
    "    latent_inputs = Input(shape=(latent_dim,))\n",
    "\n",
    "    # Determine number of filters for each transpose convolutional layer\n",
    "    filters = [f[-1] for f in encoder_layers_dim[1:-2]]\n",
    "\n",
    "    # Feed latent vector through a dense layer with ReLU activation\n",
    "    # Note that we apply the first filter in the form of dense and reshape it\n",
    "    x = Dense(dense_layer_dim, activation=\"relu\")(latent_inputs)\n",
    "    x = Reshape(first_conv_layer_dim)(x)\n",
    "\n",
    "    # Apply series of transpose convolutional layers with ReLU activation and same padding and Upsampling\n",
    "    mid_layers = [Conv2DTranspose(f, 3, activation=\"relu\", strides=2, padding=\"same\") for f in filters[::-1]]\n",
    "    for mid_layer in mid_layers:\n",
    "        x = mid_layer(x)\n",
    "\n",
    "    # Apply final convolutional layer with sigmoid activation to output reconstructed image\n",
    "    decoder_outputs = Conv2DTranspose(output_layer[-1], 3, activation=\"sigmoid\", padding=\"same\")(x)\n",
    "\n",
    "    # Create and return Keras model with latent vector as input and reconstructed image as output\n",
    "    return Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "\n",
    "\n",
    "# VAE for GrayScale Images\n",
    "class VAE(Model):\n",
    "    \"\"\"\n",
    "    This is a Variational Autoencoder (VAE) implemented using the Keras Model API. \n",
    "    It has an encoder and a decoder network defined separately and passed to the constructor as arguments. \n",
    "    The VAE class inherits from the Keras Model class and overrides the train_step() method to define the training loop.\n",
    "\n",
    "    During forward pass, the encoder takes an input image and outputs the mean and standard deviation \n",
    "    of a latent space distribution, as well as a sampled vector from that distribution. \n",
    "    The decoder takes the sampled vector and outputs a reconstructed image.\n",
    "\n",
    "    The training loop consists of computing the reconstruction loss and the \n",
    "    KL divergence loss, and then computing gradients and updating weights using the Adam optimizer. \n",
    "    The reconstruction loss measures the difference between the input image and the reconstructed image,\n",
    "    while the KL divergence loss measures the divergence between the latent space distribution and a standard normal distribution. \n",
    "    The total loss is the sum of the two losses.\n",
    "\n",
    "    The VAE class also defines three metrics to track during training: the total loss, the reconstruction loss, \n",
    "    and the KL divergence loss. These metrics are updated in the train_step() method and can be accessed via the metrics property. \n",
    "    The train_step() method returns a dictionary of these metrics.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "        # Define metrics to track during training\n",
    "        self.total_loss_tracker = Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = Mean(name=\"reconstruction_loss\")\n",
    "        self.kl_loss_tracker = Mean(name=\"kl_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "    \n",
    "    # Define forward pass\n",
    "    def call(self, x):\n",
    "        z_mean, z_log_var, z = self.encoder(x)\n",
    "        reconstruction = self.decoder(z)\n",
    "        return z_mean, z_log_var, z, reconstruction\n",
    "\n",
    "    # Define training step\n",
    "    def train_step(self, data):\n",
    "        with GradientTape() as tape:\n",
    "            # Forward pass through encoder and decoder\n",
    "            z_mean, z_log_var, z, reconstruction = self(data)\n",
    "            \n",
    "            # Compute reconstruction loss\n",
    "            reconstruction_loss = reduce_mean(\n",
    "                reduce_sum(\n",
    "                    binary_crossentropy(data, reconstruction), axis=(1, 2)\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # Compute KL divergence loss\n",
    "            kl_loss = -0.5 * (1 + z_log_var - tf_square(z_mean) - tf_exp(z_log_var))\n",
    "            kl_loss = reduce_mean(reduce_sum(kl_loss, axis=1))\n",
    "\n",
    "            # Compute total loss\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "            \n",
    "        # Compute gradients and update weights\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        \n",
    "        # Update metrics\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        \n",
    "        # Return metrics as dictionary\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }\n",
    "\n",
    "# Function to parse command line arguments\n",
    "def parse_arguments():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument('--output-image-shape', type=int, default=56)\n",
    "    parser.add_argument('--filters', type=int, nargs='+', default=[32, 64])\n",
    "    parser.add_argument('--dense-layer-dim', type=int, default=16)\n",
    "    parser.add_argument('--latent-dim', type=int, default=6)\n",
    "    parser.add_argument('--beta', type=float, default=1.0)\n",
    "    parser.add_argument('--batch-size', type=int, default=128)\n",
    "    parser.add_argument('--learning-rate', type=float, default=1e-4)\n",
    "    parser.add_argument('--patience', type=int, default=10)\n",
    "    parser.add_argument('--epochs', type=int, default=2)\n",
    "    parser.add_argument('--train-split', type=float, default=0.8)\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "\n",
    "def get_image_data(all_dirs):\n",
    "    # List to store all image file paths\n",
    "    all_image_paths = []\n",
    "\n",
    "    # Loop through all directories and subdirectories in the data directory\n",
    "    for data_dir in all_dirs:\n",
    "        for root, dirs, files in os.walk(data_dir):\n",
    "            for file in files:\n",
    "                # Check if the file is an image file (you can add more extensions as needed)\n",
    "                if file.endswith('.jpg') or file.endswith('.jpeg') or file.endswith('.png'):\n",
    "                    # If the file is an image file, append its path to the list\n",
    "                    all_image_paths.append(os.path.join(root, file))\n",
    "        print(data_dir)\n",
    "    image_count = len(all_image_paths)\n",
    "    print(\"Total number of imges:\", image_count)\n",
    "    return all_image_paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAECallback(Callback):\n",
    "    \"\"\"\n",
    "    Randomly sample 5 images from validation_data set and shows the reconstruction after each epoch\n",
    "    \"\"\"\n",
    "    def __init__(self, vae, validation_data, log_dir, n=5):\n",
    "        self.vae = vae\n",
    "        self.validation_data = validation_data\n",
    "        self.n = n\n",
    "        self.log_dir = log_dir\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Generate decoded images from the validation input\n",
    "        validation_batch = next(iter(self.validation_data))\n",
    "        _, _, _, reconstructed_images = self.vae.predict(validation_batch)\n",
    "\n",
    "        # Rescale pixel values to [0, 1]\n",
    "        reconstructed_images = np.clip(reconstructed_images, 0.0, 1.0)\n",
    "\n",
    "        # Plot the original and reconstructed images side by side\n",
    "        plt.figure(figsize=(10, 2*self.n))  # Adjusted the figure size\n",
    "        for i in range(self.n):\n",
    "            plt.subplot(self.n, 2, 2*i+1)\n",
    "            plt.imshow(validation_batch[i], cmap='gray')\n",
    "            plt.axis('off')\n",
    "            plt.subplot(self.n, 2, 2*i+2)\n",
    "            plt.imshow(reconstructed_images[i], cmap='gray')\n",
    "            plt.axis('off')\n",
    "        plt.savefig(self.log_dir + '\\\\decoded_images_epoch_{:04d}.png'.format(epoch))\n",
    "        # plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOGDIR = os.path.join(r\"C:\\Users\\kkosara\\ImageAutoEncoder\\logs\", datetime.now().strftime(\"%Y%m%d-%H%M%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(LOGDIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# args = parse_arguments()\n",
    "all_image_paths = get_image_data([r'C:\\Users\\kkosara\\ImageAutoEncoder\\Data'])\n",
    "image_count = len(all_image_paths)\n",
    "TRAIN_SPLIT  = 0.8\n",
    "OUTPUT_IMAGE_SHAPE = 56\n",
    "INPUT_SHAPE = (OUTPUT_IMAGE_SHAPE, OUTPUT_IMAGE_SHAPE, 1)\n",
    "FILTERS = [32, 64]\n",
    "DENSE_LAYER_DIM = 56 #args.dense_layer_dim\n",
    "LATENT_DIM = 8 #args.latent_dim\n",
    "BATCH_SIZE = 20 #args.batch_size\n",
    "EPOCHS = 200 #args.epochs\n",
    "LEARNING_RATE = 1e-4 #args.learning_rate\n",
    "\n",
    "df_train = pd.DataFrame({'image_paths': all_image_paths[:int(image_count*TRAIN_SPLIT)]})\n",
    "df_test = pd.DataFrame({'image_paths': all_image_paths[:int(image_count*(1-TRAIN_SPLIT))]})\n",
    "\n",
    "train_datagen_args = dict(\n",
    "    rescale=1.0 / 255,  # Normalize pixel values between 0 and 1\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    rotation_range=90,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    ")\n",
    "test_datagen_args = dict(rescale=1.0 / 255)\n",
    "\n",
    "train_datagen = ImageDataGenerator(**train_datagen_args)\n",
    "test_datagen = ImageDataGenerator(**test_datagen_args)\n",
    "# Use flow_from_dataframe to generate data batches\n",
    "train_data_generator = train_datagen.flow_from_dataframe(\n",
    "    dataframe=df_train,\n",
    "    color_mode='grayscale',\n",
    "    x_col='image_paths',\n",
    "    y_col=None,\n",
    "    target_size=(OUTPUT_IMAGE_SHAPE, OUTPUT_IMAGE_SHAPE),  # Specify the desired size of the input images\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=None,  # Set to None since there are no labels\n",
    "    shuffle=True  # Set to True for randomizing the order of the images\n",
    ")\n",
    "\n",
    "test_data_generator = test_datagen.flow_from_dataframe(\n",
    "    dataframe=df_test,\n",
    "    color_mode='grayscale',\n",
    "    x_col='image_paths',\n",
    "    y_col=None,\n",
    "    target_size=(OUTPUT_IMAGE_SHAPE, OUTPUT_IMAGE_SHAPE),  # Specify the desired size of the input images\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=None,  # Set to None since there are no labels\n",
    "    shuffle=True  # Set to True for randomizing the order of the images\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "encoder, encoder_layers_dim = encoder_model(input_shape = INPUT_SHAPE, filters=FILTERS, dense_layer_dim=DENSE_LAYER_DIM, latent_dim=LATENT_DIM)\n",
    "print(encoder.summary())\n",
    "print(encoder_layers_dim)\n",
    "decoder = decoder_model(encoder_layers_dim)\n",
    "print(decoder.summary())\n",
    "vae = VAE(encoder, decoder)\n",
    "vae.compile(optimizer=Adam(learning_rate=LEARNING_RATE))\n",
    "vae_callback = VAECallback(vae, test_data_generator, LOGDIR)\n",
    "\n",
    "\n",
    "# # Create a callback that saves the model's weights\n",
    "# checkpoint_cb = ModelCheckpoint(filepath=LOGDIR,\n",
    "#                                 save_weights_only=True,\n",
    "#                                 monitor='mse',\n",
    "#                                 mode='min',\n",
    "#                                 save_best_only=True,\n",
    "#                                 verbose=1)\n",
    "\n",
    "# # Create a callback that will stop the training when there is no improvement in\n",
    "# # the validation loss for 10 consecutive epochs\n",
    "# early_stopping_cb = EarlyStopping(monitor='mse', \n",
    "#                                   patience=10,\n",
    "#                                   mode='min',\n",
    "#                                   verbose=1,\n",
    "#                                   restore_best_weights=True)\n",
    "\n",
    "# Create a TensorBoard callback\n",
    "tensorboard_cb = TensorBoard(log_dir=LOGDIR, histogram_freq=1)\n",
    "\n",
    "# Combine all the callbacks in a list\n",
    "callbacks = [tensorboard_cb, vae_callback]\n",
    "\n",
    "# Pass the list of callbacks to the fit method\n",
    "# history = model.fit(..., callbacks=callbacks)\n",
    "\n",
    "history = vae.fit(\n",
    "    train_data_generator,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=test_data_generator,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "encoder.save(LOGDIR +\"/encoder\", overwrite=True, save_format=None)\n",
    "decoder.save(LOGDIR +\"/decoder\", overwrite=True, save_format=None)\n",
    "vae.save(LOGDIR +\"/vae\", overwrite=True, save_format=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_baseline_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
